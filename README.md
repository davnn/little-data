**Supplementary material**

```
@article{muhrLittleDataOften2022,
  title = {Little Data Is Often Enough for Distance-Based Outlier Detection},
  author = {Muhr, David and Affenzeller, Michael},
  year = {2022},
  month = jan,
  journal = {Procedia Computer Science},
  series = {3rd {{International Conference}} on {{Industry}} 4.0 and {{Smart Manufacturing}}},
  volume = {200},
  pages = {984--992},
  issn = {1877-0509},
  doi = {10.1016/j.procs.2022.01.297},
  abstract = {Many real-world use cases benefit from fast training and prediction times, and much research went into speeding up distance-based outlier detection methods to millions of data points. Contrary to popular belief, our findings suggest that little data is often enough for distance-based outlier detection models. We show that using only a tiny fraction of the data to train distance-based outlier detection models often leads to no significant reduction in predictive performance and detection variance over a wide range of tabular datasets. Furthermore, we compare a data reduction based on random subsampling and clustering-based prototypes and show that both approaches yield similar outlier detection results. Simple random subsampling, thus, proves to be a useful benchmark and baseline for future research on speeding up distance-based outlier detection.},
  langid = {english},
  keywords = {anomaly detection,clustering,k-means,knn,local outlier factor,lof,nearest neighbors,outlier detection,prototypes,unsupervised}
}
```
